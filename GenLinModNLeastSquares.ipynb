{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2963d50-04c4-49ca-a890-c853d044a21d",
   "metadata": {},
   "source": [
    "# General Linear Models and Least Squares\n",
    "A key distinction of *statistical* models is that they contain free parameters that are fit to data. \n",
    "\n",
    "eg- I know the stock market will go up over time, but I dont know by how much. Therefore I allow the change in stock market price over time to be a free parameter whose numerical value is determined by data.\n",
    "\n",
    "Crafting a statistical model can be difficult, but finding the free parameters based on fitting the model to data is a simple matter of linear algebra- we just have to put the pieces together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521394b5-f91d-444d-ac2e-cb8679e95905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sl\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24759607-b5ca-450e-b6a1-b2b77ab2ee49",
   "metadata": {},
   "source": [
    "## General Linear Models\n",
    "A statistical model is a set of equations that relates predictors (called *independent variables*) to observations (called the *dependent variables*). In the model of the stock market, the independent variable is *time* and the dependent variable is *stock market price* (the S&P 500 index).\n",
    "\n",
    "Here we will focus on general linear models, or GLMs. Regression is a type of GLM.\n",
    "\n",
    "### Terminology\n",
    "Statisticians use slightly different terminology. The following table shows the key letters and descriptions for vectors and matrices used in GLM.\n",
    "\n",
    "| LinAlg | Stats | Description|\n",
    "|--------|-------|-------------------------------------------|\n",
    "|$\\mathbf{A}x=b$|$\\mathbf{X}\\beta=y$|General Linear Model(GLM)|\n",
    "|$\\mathbf{A}$|$\\mathbf{X}$|Design matrix (columns=independent vars, predictors, regressors)|\n",
    "|$x$|$\\beta$|Regression coefficients or beta parameters|\n",
    "|$b$|$y$|Dependent variable, outcome measure, data|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb003f4b-4e77-43cb-ab6d-49b288d914de",
   "metadata": {},
   "source": [
    "### Setting up a GLM\n",
    "Setting up a GLM involves:\n",
    "1. defining an equation that relates the predictor variables to the dependent variable\n",
    "2. mapping the observed data onto the equations,\n",
    "3. transforming the series of equations into a matrix equation\n",
    "4. solving that equation.\n",
    "\n",
    "We will use a simple example. Here is a model that predicts adult height based on weight and on parent's height. The equation looks like:\n",
    "\n",
    "$y = \\beta_0+\\beta_1 w+\\beta_2 h +\\epsilon$\n",
    "\n",
    "$y$ is the height of the individual, $w$ is their weight, and $h$ is the average of the parents heights. $\\epsilon$ is an error term (also called a residual) because we cannot reasonably expect that weight and height perfectly determine an individual's height. All the unknown factors in the height calculation are absorbed by this error term.\n",
    "\n",
    "The hypothesis is that weight and parents' height are important for an individual's height, but we don't know *how* important each variable is. the $\\beta$s terms/coefficients/weights tell us how to combine weight and parent's height to predict an individual's height. In other words, it's a linear combination, where the $\\beta$s are weights.\n",
    "\n",
    "$\\beta_0$ is called an *intercept*. The intercept term is a vector of all 1s. Without an intercept term, the best-fit line is forced to pass through the origin.\n",
    "\n",
    "Now that we have an equation, we need to map the observed data onto it. We will use the following data:\n",
    "\n",
    "|$y$|$w$|$h$|\n",
    "|---|---|---|\n",
    "|175|70|177|\n",
    "|181|86|190|\n",
    "|159|63|180|\n",
    "|165|62|172|\n",
    "\n",
    "Mapping the observed data onto our statistical model involves replicating the equation four times (corresponding to four observations in our dataset), each time replacing the variables $y,w,h$ with the measured data:\n",
    "\n",
    "$\\begin{array}c175=\\beta_0+70\\beta+177\\beta_2\\\\181=\\beta_0+86\\beta_1+190\\beta_2\\\\159=\\beta_0+63\\beta_1+180\\beta_2\\\\165=\\beta_0+62\\beta_1+172\\beta_2\\end{array}$\n",
    "\n",
    "the $\\epsilon$ term is omitted for now; We now translate this system of equations into a matrix equation.\n",
    "\n",
    "$\\begin{bmatrix}1&70&177\\\\1&86&190\\\\1&63&180\\\\1&62&172\\end{bmatrix}\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\end{bmatrix}=\\begin{bmatrix}175\\\\181\\\\159\\\\165\\end{bmatrix}$\n",
    "\n",
    "We can express this term as $\\mathbf{X}\\beta=y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6997825-91db-4d74-a312-63c3d94364e5",
   "metadata": {},
   "source": [
    "## Solving GLMs\n",
    "the main idea of this section is to solve for the vector of unknown coefficients $\\beta$. Simply left-multiply both sides by the left-inverse of $\\mathbf{X}$, the design matrix\n",
    "\n",
    "The solution is:\n",
    "\n",
    "$\\mathbf{X}\\beta=y$<br>\n",
    "$(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\beta=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$<br>\n",
    "$\\beta=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$\n",
    "\n",
    "The final equation is called the *least squares solution* and is one of the most important mathematical equations in applied linear algebra.\n",
    "\n",
    "There may be variations:<br>\n",
    "$b=(\\mathbf{H}^T\\mathbf{WH}+\\lambda\\mathbf{L}^T\\mathbf{L})^{-1}\\mathbf{H}^Tx$\n",
    "\n",
    "We can see the least squares formula embedded in the above formula.\n",
    "\n",
    "The least squares solution via the left-inverse can be translated directly into Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6482e3b-c6ad-4236-84be-f05ec82610ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.82121026e-13]\n",
      " [ 1.00000000e+00]\n",
      " [-7.63833441e-14]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([ [1,175,70],\n",
    "              [1,181,86],\n",
    "              [1,159,63],\n",
    "              [1,165,62] ])\n",
    "y = np.array([ [175,181,159,165] ]).T\n",
    "\n",
    "#Xb=y\n",
    "#(XtX)-1Xt b = (XtX)-1Xty\n",
    "X_leftinv = np.linalg.inv(X.T@X)@X.T\n",
    "\n",
    "beta = X_leftinv @ y\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9b873-8e7e-4ffa-a5d6-814aad7eafcb",
   "metadata": {},
   "source": [
    "## Is the solution Exact?\n",
    "The equation $\\mathbf{X}\\beta=y$ is exactly solvable when $y$ is in the column space of the design matrix $\\mathbf{X}$. So the question is whether the data vector is guaranteed to be in the column space of the statistical model. There is no such guarantee. In fact, the data vector $y$ is almost never in the column space of $\\mathbf{X}$.\n",
    "\n",
    "If the data is in the column space of the matrix, it means that the model accounts for 100% of the variance in the data. But this almost never happens: real-world data contains noise and sampling, and the models are simplifications that do not account for all of the variability.\n",
    "\n",
    "The solution to the conundrum is that we modify the GLM equation to allow for a discrepancy between the model-predicted data and the observed data. It can be expressed in several equivalent ways:\n",
    "\n",
    "$\\mathbf{X}\\beta=y+\\epsilon$<br>\n",
    "$\\mathbf{X}\\beta+\\epsilon=y$<br>\n",
    "$\\epsilon = \\mathbf{X}\\beta-y$\n",
    "\n",
    "The interpretation of the first equation is that $\\epsilon$ is a residual or an error term that you add to the data vector so that it fits inside the column space of the design matrix. \n",
    "\n",
    "The interpretation of the second equation is that the residual term is an adjustment to the design matrix so that it fits the data perfectly.\n",
    "\n",
    "Finally, the interpretation of the third equation is that the residual is defined as the difference between the model-predicted data and the observed data.\n",
    "\n",
    "The point of this section is that the observed data is almost never inside the subspace spanned by the regressors. For this reason its also common to see the GLM expressed as $\\mathbf{X}=\\beta\\hat{y}$ where $\\hat{y}=y+\\epsilon$.\n",
    "\n",
    "Therefore the goal of the GLM is to find the linear combination of the regressors that gets as close as possible to the observed data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19344584-371b-42c7-b60e-8fb1d414c56a",
   "metadata": {},
   "source": [
    "## Geometric Perspective on Least Squares\n",
    "Consider the column space of the design matrix $C(\\mathbf{X})$ is a subspace of $\\mathbb{R}^M$. It's typically a very low-dimensional subspace (that is, $N << M$), because statistical models tend to have many more observations than predictors. The dependent variable is a vector $y\\in \\mathbb{R}^M$. The questions at hand are, is the vector $y$ in the column space of the design matrix, and if not, what coordinate inside the column space of the design matrix is as close as possible to the data vector.\n",
    "\n",
    "The answer to the first question is - no. The second has already been discussed in Chap2.\n",
    "\n",
    "Our goal is to find the set of coefficients $\\beta$ such that the weighted combination of columns in $\\mathbf{X}$ minimizes the distance to data vector $y$. We can call that projection vector $\\epsilon$. How do we find the vector $\\epsilon$ and the coefficients $\\beta$? We can use orthogonal vector projection. \n",
    "\n",
    "The key insight is that the shortest distance between $y$ and $\\mathbf{X}$ is given by the projection vector $y-\\mathbf{X}\\beta$ that meets $\\mathbf{X}$ at a right angle.\n",
    "\n",
    "So, we can use matrix-vector multiplication to arrive at:\n",
    "\n",
    "$\\mathbf{X}^T\\epsilon=0$<br>\n",
    "$\\mathbf{X}^T(y-\\mathbf{X}\\beta)=0$<br>\n",
    "$\\mathbf{X}^Ty-\\mathbf{X}^T\\mathbf{X}\\beta = 0$<br>\n",
    "$\\mathbf{X}^T\\mathbf{X}\\beta = \\mathbf{X}^Ty$<br>\n",
    "$\\beta = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$\n",
    "\n",
    "We started by thinking about the GLM as a geometric projection of a data vector onto the column space of a design matrix, applied the principle of orthogonal vector projection, and have re-derived the same left-inverse solution that we got from the algebraic approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f86ea12-cbf4-42a5-8ab3-92ef97264404",
   "metadata": {},
   "source": [
    "## Why Does Least Squares work?\n",
    "The 'squares' here refer to squared errors between the predicted and the observed data. There is an error term for each $i$th predicted data point, which is defined as $\\epsilon_i = \\mathbf{X}_i\\beta-y_i$. \n",
    "\n",
    "the errors are all captured in one vector:\n",
    "\n",
    "$\\epsilon= \\mathbf{X}\\beta - y$\n",
    "\n",
    "If the model is a good fit for the data, then the errors should be small. Therefore, we can say that the objective of model fitting is to select the elements for $\\beta$ to minimize the elements in $\\epsilon$. But just *minimizing* the errors would cause the model to predict values towards negative infinity. Thus, instead, we minimize the *squared* errors, which correspond to their geometric squared distance to the observed data $y$, regardless of whether the prediction itself is positive or negative. This is the same thing as minimizing the squared norm of the errors, hence the name \"Least Squares\".\n",
    "\n",
    "this leads to this modification:\n",
    "\n",
    "$\\lVert\\epsilon\\rVert^2=\\lVert\\mathbf{X}\\beta-y\\rVert^2$\n",
    "\n",
    "This can be seen as an optimization problem, as in we want to find the set of coefficients that minimizes the squared errors. this can be expressed as follows:\n",
    "\n",
    "$\\min(\\beta)\\lVert\\mathbf{X}\\beta-y\\rVert^2$\n",
    "\n",
    "The solution to this optimization can be found by setting the derivative of the objective to zero and applying some differential calculus and some algebra.\n",
    "\n",
    "$0 = \\frac{d}{d\\beta}\\lVert\\mathbf{X}\\beta-y\\rVert^2=2\\mathbf{X}^T(\\mathbf{X}\\beta-y)$<br>\n",
    "$0 = \\mathbf{X}^T\\mathbf{X}\\beta-\\mathbf{X}^Ty$<br>\n",
    "$\\mathbf{X}^T\\mathbf{X}\\beta=\\mathbf{X}^Ty$<br>\n",
    "$\\beta=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$\n",
    "\n",
    "Again, we started from a different perspective- minimize the squared distance between the model-predicted values and the observed values- again we reach the same solution to least squares that we reached simply by using linear algebra intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276897bb-e4ba-4efe-92d3-9535595d9181",
   "metadata": {},
   "source": [
    "## Least Squares via QR\n",
    "The left inverse method is theoretically sound, but risks numerical instability. This is partly because it requires computing the matrix inverse, which can be a numerically unstable operation.\n",
    "\n",
    "But it turns out the matrix $\\mathbf{X}^T\\mathbf{X}$ itself can introduce difficulty. Multiplying a matrix by its transpose has implications for properties such as as the norm and the condition number. (matrices with high condition numbers can be more numerically unstable when squared).\n",
    "\n",
    "QR decomposition provides a more stable way to solve the least squares problem.\n",
    "\n",
    "$\\mathbf{X}\\beta=y$<br>\n",
    "$\\mathbf{QR}\\beta=y$<br>\n",
    "$\\mathbf{R}\\beta = \\mathbf{Q}^Ty$<br>\n",
    "$\\beta = \\mathbf{R}^{-1}\\mathbf{Q}^Ty$<br>\n",
    "\n",
    "These equations are slightly simplified from the actual low-level numerical implementations. For eg, $\\mathbf{R}$ is the same shape as $\\mathbf{X}$ i.e tall (therefore, non-invertible), although only the first $N$ rows are non-zero. Which means that rows $N+1$ through $M$ do not contribute to the solution. Those rows can be removed from $\\mathbf{R}$ and from $\\mathbf{Q}^Ty$. \n",
    "\n",
    "It's not even necessary to invert $\\mathbf{R}$- that matrix is upper-triangular, and therefore the solution can be obtained via back-substitution. It's the same as solving a system of equations via the Gauss-Jordan method: augment the coefficients matrix by the constants, row-reduce to obtain the RREF, and extract the solution from the final column of the augmented matrix.\n",
    "\n",
    "The conclusion here is that QR decomposition solves the least squares problem without squaring $\\mathbf{X}^T\\mathbf{X}$ and without explicitly inverting a matrix. The main reisk of numerical instability comes from computing $\\mathbf{Q}$, but this is stable when done through Householder reflections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
